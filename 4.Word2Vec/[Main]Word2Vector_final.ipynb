{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 워드투벡터\n",
    "- https://wikidocs.net/22660"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cmd 명령어\n",
    "- https://dumps.wikimedia.org/kowiki/latest/ 에서 덤프파일 다운로드 (kowiki-latest-pages-articles.xml.bz2)\n",
    "- https://github.com/attardi/wikiextractor 에서 zip파일 다운로드한 뒤 덤프bz2파일과 동일한 경로에 풀기\n",
    "- cmd에서 경로 찾아간 뒤 압축해제 명령어 <br>\n",
    "(python WikiExtractor.py kowiki-latest-pages-articles.xml.bz2 => 약 10분정도 걸림 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추출된 text데이터 합치기\n",
    "- copy AA디렉토리의 경로\\wiki* wikiAA.txt\n",
    ">copy AB디렉토리의 경로\\wiki* wikiAB.txt <br>\n",
    "copy AC디렉토리의 경로\\wiki* wikiAC.txt <br>\n",
    "copy AD디렉토리의 경로\\wiki* wikiAD.txt <br>\n",
    "copy AE디렉토리의 경로\\wiki* wikiAE.txt <br>\n",
    "copy AF디렉토리의 경로\\wiki* wikiAF.txt\n",
    "- copy 현재 디렉토리의 경로\\wikiA* wiki_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# txt데이터 읽어오기\n",
    "- 직접하기엔 kowiki 전처리 + mecab ko dic부분 처리가 곤란\n",
    "- 18년 7월에 올라온 데이터 임포트 https://github.com/roboreport/doc2vec-api\n",
    "- 혼자 직접해보려면 https://www.lucypark.kr/courses/2015-dm/text-mining.html 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선지자 코드로 kowiki데이터 전처리(앞부분)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "\n",
    "\n",
    "# 형태소 분석된 라인을 wakati 파일에 append하는 함수\n",
    "def append_to_file(input, file):\n",
    "    with open(file, 'a', encoding='utf-8') as writeFp:\n",
    "        writeFp.write(input + \" \")\n",
    "\n",
    "\n",
    "wakati_file = \"wiki_pre_ok\"     # wakati 파일 이름\n",
    "line_cnt = 0                    # 위키 텍스트 라인 인덱스 변수\n",
    "twitter = Twitter()             # 형태소 분석기 인스턴스\n",
    "\n",
    "\n",
    "# with open(wakati_file, 'a', encoding='utf-8') as writeFp:\n",
    "# 파일 열기\n",
    "with open('wiki_data.txt', 'r', encoding='utf-8') as readFp:\n",
    "    for line in readFp:\n",
    "        if not line:\n",
    "            print(\"\\n>> [INFO] not line, break : line_cnt=%.f\" % line_cnt)\n",
    "            break\n",
    "\n",
    "        if line_cnt % 1000 == 0:\n",
    "            print(\"current - \" + str(line_cnt))\n",
    "\n",
    "        # 라인 단위 형태소 분석\n",
    "        malist = twitter.pos(line, norm=True, stem=True)\n",
    "\n",
    "        # 필요한 어구만 대상으로 하기\n",
    "        for word in malist:\n",
    "            if not word[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "                try:\n",
    "                    append_to_file(word[0], wakati_file)\n",
    "                    # writeFp.write(word[0] + \" \")\n",
    "                except:\n",
    "                    pass\n",
    "        line_cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뒷부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "\n",
    "\n",
    "# 형태소 분석된 라인을 wakati 파일에 append하는 함수\n",
    "def append_to_file(input, file):\n",
    "    with open(file, 'a', encoding='utf-8') as writeFp:\n",
    "        writeFp.write(input + \" \")\n",
    "\n",
    "\n",
    "wakati_file = \"wiki_pre_ok33333\"     # wakati 파일 이름\n",
    "line_cnt = 0                    # 위키 텍스트 라인 인덱스 변수\n",
    "twitter = Twitter()             # 형태소 분석기 인스턴스\n",
    "\n",
    "\n",
    "# with open(wakati_file, 'a', encoding='utf-8') as writeFp:\n",
    "# 파일 열기\n",
    "with open('File2_', 'r', encoding='utf-8') as readFp:\n",
    "    for line in readFp:\n",
    "        if not line:\n",
    "            print(\"\\n>> [INFO] not line, break : line_cnt=%.f\" % line_cnt)\n",
    "            break\n",
    "\n",
    "        if line_cnt % 1000 == 0:\n",
    "            print(\"current - \" + str(line_cnt))\n",
    "\n",
    "        # 라인 단위 형태소 분석\n",
    "        malist = twitter.pos(line, norm=True, stem=True)\n",
    "\n",
    "        # 필요한 어구만 대상으로 하기\n",
    "        for word in malist:\n",
    "            if not word[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "                try:\n",
    "                    append_to_file(word[0], wakati_file)\n",
    "                    # writeFp.write(word[0] + \" \")\n",
    "                except:\n",
    "                    pass\n",
    "        line_cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 데이터로 W2V 학습\n",
    "- 파라미터 설명 : https://radimrehurek.com/gensim/models/base_any2vec.html#gensim.models.base_any2vec.BaseWordEmbeddingsModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> [INFO] Making Word2Vec model finished!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "한국어 위키백과 텍스트 데이터를 이용한 word2vec 학습 - (2)\n",
    "    - wakati 파일을 이용하여 word2vec 모델 학습시키기\n",
    "\"\"\"\n",
    "import multiprocessing\n",
    "from gensim.models import word2vec\n",
    "\n",
    "wakati_file = 'wiki_pre_concat'\n",
    "model_file = 'sunjija_wiki.model'\n",
    "\n",
    "data = word2vec.Text8Corpus(wakati_file)\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    'min_count': 5,  # 등장 횟수가 5 이하인 단어는 무시\n",
    "    'size': 300,  # 300차원짜리 벡터스페이스에 embedding\n",
    "    'sg': 1,  # 0이면 CBOW, 1이면 skip-gram을 사용한다\n",
    "    'batch_words': 10000,  # 사전을 구축할때 한번에 읽을 단어 수\n",
    "    'iter': 10,  # 보통 딥러닝에서 말하는 epoch과 비슷한, 반복 횟수\n",
    "    'workers': multiprocessing.cpu_count(),\n",
    "}\n",
    "\n",
    "model = word2vec.Word2Vec(data, **config)\n",
    "model.save(model_file)\n",
    "\n",
    "print(\"\\n>> [INFO] Making Word2Vec model finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파일 잘라내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"File_\" , \"r\", encoding = 'utf-8') as bigfile:\n",
    "    reader = bigfile.read()\n",
    "    for i, part in enumerate(reader.split(\"wiki?curid=1948248\")):\n",
    "        with open(\"File2_\", mode = \"w\", encoding = 'utf-8') as newfile:\n",
    "            newfile.write(\"wiki?curid=1948248\"+part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"File2_\", \"r\", encoding = 'utf-8') as bigfile:\n",
    "    reader = bigfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wiki_pre_ok22222\" , \"r\", encoding = 'utf-8') as bigfile:\n",
    "    reader_original = bigfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_original[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리한 파일 합치기1\n",
    "- 임포트한 파일 + 해서 합친 뒤 새 변수에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wiki_pre_ok11111\" , \"r\", encoding = 'utf-8') as bigfile1:\n",
    "    text1 = bigfile1.read()\n",
    "with open(\"wiki_pre_ok22222\" , \"r\", encoding = 'utf-8') as bigfile2:\n",
    "    text2 = bigfile2.read()\n",
    "with open(\"wiki_pre_ok33333\" , \"r\", encoding = 'utf-8') as bigfile3:\n",
    "    text3 = bigfile3.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 앨런 윌리엄 종교 단체 구성원 들 함께 살다 서로 형제 또는 자매 부르다 것 이다 수 있다 또한 이 들 서로 친숙하다 지내다 성적 교제 가지다 것 아니다 수 있다 주장 하다 그러나 시간 지나다 이러하다 균형 되다 태도 유지 하다 위 하다 점점 더 많다 노력 필요하다 되어다 그 결과 이레나이우스 비난 하다 일이 일어나다 되어다 수도 있다 주장 하다 \\n \\n '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Marcion of Sinope Greek Μαρκίων 약 85년 약 160년 은 구약성경 하나님 신약성경 하나님 전혀 다른 분 말 하다 구약 하나님 폭력 복수 잔인하다 하나님 신약 하나님 사랑 정의 용서 신 말 하다 영 지 주의 적 사상 그 들 깊다 나타나다 이원론 사상 빠지다 있다 그러나 3 세기 서구 있다 마르키온 팔다 마니교 흡수 되어다 쇠퇴하다 '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'리스 산 \\n 메다 리스 산 \\n \\n 메다 리스 산 또는 트라이아콘 탄산 은 화학식 CH CH COOH 인 30 개 탄소 구성 되다 기다 사슬 포화 지방산 \\n \\n 메다 리스 산 벌 유혹 하다 꽃 꿀 풍부하다 때문 벌 의미 하다 그리스어 melissa 그 이름 유래 되어다 \\n \\n n 트라이아콘탄산 블 레이 버그 Bleyberg 울리치 Ulrich G M 로빈슨 '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wiki curid 1948248 title 메다 리스 산 \\n 메다 리스 산 \\n \\n 메다 리스 산 또는 트라이아콘 탄산 은 화학식 CH CH COOH 인 30 개 탄소 구성 되다 기다 사슬 포화 지방산 \\n \\n 메다 리스 산 벌 유혹 하다 꽃 꿀 풍부하다 때문 벌 의미 하다 그리스어 melissa 그 이름 유래 되어다 \\n \\n n 트라이아콘탄산 블 레이 버그 '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_all = text1+text2+text3\n",
    "print(text_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11685409\n",
      "4230243\n",
      "11441203\n",
      "27356855\n"
     ]
    }
   ],
   "source": [
    "print(len(text1))\n",
    "print(len(text2))\n",
    "print(len(text3))\n",
    "print(len(text_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki_pre_concat_ver2', 'w', encoding = 'utf-8') as outfile:\n",
    "    outfile.write(text_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki_pre_concat_ver2', 'r', encoding = 'utf-8') as f:\n",
    "    text_test = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 합친파일 저장하기2\n",
    "- 임포트 없이 파일 하나로 합쳐버리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['wiki_pre_ok11111', 'wiki_pre_ok22222', 'wiki_pre_ok33333']\n",
    "with open('wiki_pre_concat', 'w', encoding = 'utf-8') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname, encoding = 'utf-8') as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wiki_pre_concat\" , \"r\", encoding = 'utf-8') as fp:\n",
    "    text_all2 = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'스앤젤레스 북쪽 모하비 사막 82.7 km 고도 올라가다 미국 공군 NASA 늘다 약 80.5 km 를 우주 경계 인정 하다 이 따르다 아마존닷컴 창업 자 제프 베 조스 블루 오리진 일 론 머스크 스페이스 X 이다 세 번 째 우주 경험 민간 기업 되어다 버진 갤럭틱 영국 버진그룹 창업 자 억만장자 리처드 브랜슨 설립 우주로켓 기업 \\n \\n \\n doc \\n \\x1a '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_all2[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27356855"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_all2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 업데이트\n",
    "- https://markroxor.github.io/gensim/static/notebooks/online_w2v_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.word2vec import Word2Vec, LineSentence\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 일단 뉴스데이터 50개 1년치 임포트 후 str으로 추출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.build_vocab(newwiki, update=True)\n",
    "model.train(newwiki)\n",
    "model.save('newmodel')\n",
    "# model = Word2Vec.load('newmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ['oldmodel', 'model']:\n",
    "    print('The vocabulary size of the', m, 'is', len(eval(m).vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pprint(model.most_similar('babymetal'))\n",
    "except KeyError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습한 모델 불러오기 + 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec.load('wiki.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('記述', 0.5674477219581604),\n",
       " ('나노기술', 0.5489809513092041),\n",
       " ('생명공학', 0.5480045080184937),\n",
       " ('사물인터넷', 0.5399839878082275),\n",
       " ('정보기술', 0.5376924276351929),\n",
       " ('EVM', 0.5327778458595276),\n",
       " ('집적회로', 0.5318139791488647),\n",
       " ('공기역학', 0.5303913950920105),\n",
       " ('재료공학', 0.5286709666252136),\n",
       " ('빅데이터', 0.5267904996871948)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"기술\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sbg02\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.16007838,  0.12620604,  0.2777441 , -0.2612181 , -0.01790356,\n",
       "       -0.2089102 , -0.1675327 , -0.4280348 ,  0.27242246, -0.14717534,\n",
       "       -0.29484496,  0.20597458, -0.15454775,  0.17009403,  0.09165443,\n",
       "       -0.0742473 ,  0.21401405,  0.18405649,  0.11912371,  0.14039603,\n",
       "        0.24302718,  0.01234212,  0.2290771 ,  0.0734644 , -0.0798612 ,\n",
       "        0.23556091, -0.2395627 ,  0.10925279,  0.02061079,  0.05275683,\n",
       "        0.3321623 , -0.20076627,  0.06888626,  0.22496831,  0.38358772,\n",
       "       -0.16497424, -0.3706422 , -0.05479194,  0.05750748,  0.01291937,\n",
       "       -0.39333573, -0.02760107,  0.25392723, -0.22085704,  0.12813307,\n",
       "        0.44810957, -0.16438305, -0.08906318, -0.61589587,  0.22605628,\n",
       "        0.17189343,  0.25962237, -0.09697343,  0.835539  , -0.15276985,\n",
       "       -0.24466874,  0.34055516, -0.08446137,  0.4887809 , -0.14466265,\n",
       "       -0.15592583, -0.12190037, -0.2725619 ,  0.29461765,  0.13410264,\n",
       "       -0.40681398,  0.29290962,  0.08461123,  0.08659571, -0.20259911,\n",
       "        0.21504757,  0.02792188, -0.09830555, -0.26854688, -0.34534672,\n",
       "        0.14247085,  0.4989013 , -0.17161651, -0.3438228 , -0.17711009,\n",
       "       -0.07139224, -0.5369354 ,  0.01612214,  0.11351462,  0.15228137,\n",
       "       -0.4927144 ,  0.2955477 ,  0.0522016 ,  0.12286793, -0.19868925,\n",
       "        0.19513811, -0.30712953, -0.16301097,  0.181414  ,  0.13043346,\n",
       "       -0.07014781,  0.09067032, -0.1528085 , -0.21145578, -0.24717246,\n",
       "       -0.0372688 ,  0.4367019 , -0.05155522, -0.29854   ,  0.01632737,\n",
       "       -0.13194792, -0.15163894,  0.00705234, -0.03798179,  0.02858916,\n",
       "       -0.58739877, -0.16598058,  0.7991499 ,  0.01833093,  0.37272742,\n",
       "        0.03443254, -0.2338659 , -0.18935144,  0.2785833 ,  0.18509002,\n",
       "        0.2835196 ,  0.33195746,  0.46379638,  0.4969495 ,  0.05980626,\n",
       "       -0.09337755,  0.08651727,  0.1711358 , -0.32334268, -0.2735725 ,\n",
       "        0.52488947, -0.05893099,  0.31779572, -0.07806645, -0.0747754 ,\n",
       "        0.12174094, -0.31018454, -0.00140167, -0.7303411 , -0.25172427,\n",
       "        0.17795111, -0.0536669 ,  0.3526252 , -0.31591398, -0.10495966,\n",
       "       -0.2911846 ,  0.22590345, -0.00646378, -0.29523152, -0.15971318,\n",
       "       -0.44582257, -0.02568937,  0.20922044,  0.17663759, -0.25341693,\n",
       "       -0.23914677, -0.05331451,  0.16211307, -0.19134764,  0.06766582,\n",
       "        0.08960928,  0.11574177, -0.07478718, -0.1385823 ,  0.23916999,\n",
       "       -0.04954436, -0.26659828,  0.04697238,  0.28198877, -0.07882889,\n",
       "       -0.06376993, -0.00931324,  0.24562705,  0.44602573,  0.01537292,\n",
       "       -0.14861812, -0.21011598,  0.10202852,  0.7277781 ,  0.2788912 ,\n",
       "       -0.46212864, -0.4290236 ,  0.2820802 , -0.00694595, -0.31989807,\n",
       "        0.17775694, -0.00702582,  0.16418882,  0.44556338, -0.28946272,\n",
       "        0.25133726, -0.16868852,  0.00390241, -0.16135897,  0.3319411 ,\n",
       "       -0.37357324,  0.08075865, -0.2593427 ,  0.17707808,  0.0387362 ,\n",
       "       -0.16176823,  0.21027665, -0.35876286,  0.7802985 , -0.24378896,\n",
       "       -0.22323985, -0.09397312, -0.614562  , -0.07379443,  0.15682763,\n",
       "       -0.03980799, -0.16124569, -0.25030568,  0.08776979, -0.24842258,\n",
       "        0.11151925, -0.5681263 ,  0.16043873, -0.6148028 ,  0.11770176,\n",
       "       -0.15839842, -0.06000234,  0.44193196, -0.24622156, -0.05413346,\n",
       "       -0.18590873, -0.12563665, -0.4494798 , -0.08201786, -0.21553582,\n",
       "        0.52558994, -0.27844885,  0.32585698, -0.21563146,  0.29726976,\n",
       "       -0.22814144,  0.01504872, -0.160003  , -0.24528104, -0.02833027,\n",
       "       -0.20353544, -0.02921725, -0.06893373,  0.19325192,  0.5360095 ,\n",
       "       -0.4206408 ,  0.25909057,  0.3955326 , -0.14772286,  0.1367527 ,\n",
       "       -0.24879174, -0.16309553, -0.134173  ,  0.00517254, -0.21190566,\n",
       "       -0.26811314, -0.3280582 ,  0.3813616 ,  0.15768681,  0.03729457,\n",
       "       -0.0728693 , -0.33303404,  0.38105813,  0.22018813, -0.21281299,\n",
       "        0.29759914, -0.03466683,  0.25846127,  0.22712477, -0.04121077,\n",
       "        0.2568216 , -0.15489936,  0.2834281 ,  0.04217341, -0.2911135 ,\n",
       "        0.74936783,  0.37200055, -0.19366157,  0.04132012,  0.23410131,\n",
       "        0.3588683 , -0.1348515 , -0.42947447,  0.27582976,  0.06051109,\n",
       "        0.5093826 ,  0.15843128,  0.14612164,  0.02894791, -0.01187134,\n",
       "       -0.10720804, -0.2700785 , -0.10608228,  0.08981162,  0.07044974,\n",
       "        0.1419813 ,  0.3128484 , -0.15698501, -0.0190399 ,  0.3807885 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['주식']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-7489d91bee3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'news_all.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('news_all.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "\n",
    "\n",
    "# 형태소 분석된 라인을 wakati 파일에 append하는 함수\n",
    "def append_to_file(input, file):\n",
    "    with open(file, 'a', encoding='utf-8') as writeFp:\n",
    "        writeFp.write(input + \" \")\n",
    "\n",
    "\n",
    "wakati_file = \"wiki_pre_ok\"     # wakati 파일 이름\n",
    "line_cnt = 0                    # 위키 텍스트 라인 인덱스 변수\n",
    "twitter = Twitter()             # 형태소 분석기 인스턴스\n",
    "\n",
    "\n",
    "# with open(wakati_file, 'a', encoding='utf-8') as writeFp:\n",
    "# 파일 열기\n",
    "with open('wiki_data.txt', 'r', encoding='utf-8') as readFp:\n",
    "    for line in readFp:\n",
    "        if not line:\n",
    "            print(\"\\n>> [INFO] not line, break : line_cnt=%.f\" % line_cnt)\n",
    "            break\n",
    "\n",
    "        if line_cnt % 1000 == 0:\n",
    "            print(\"current - \" + str(line_cnt))\n",
    "\n",
    "        # 라인 단위 형태소 분석\n",
    "        malist = twitter.pos(line, norm=True, stem=True)\n",
    "\n",
    "        # 필요한 어구만 대상으로 하기\n",
    "        for word in malist:\n",
    "            if not word[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "                try:\n",
    "                    append_to_file(word[0], wakati_file)\n",
    "                    # writeFp.write(word[0] + \" \")\n",
    "                except:\n",
    "                    pass\n",
    "        line_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "벡터값으로 표현해야 하는데\n",
    "닥투벡을 한 문장의 벡터 차원이 작아진다.\n",
    "-tf /idf의 영역이니 거기로 들어간다.\n",
    "\n",
    "우리는 여기서 벡터만 뽑아낸 것.\n",
    "\n",
    "신규데이터도 워투벡으로 정제해서\n",
    "워투벡으로 벡터를 뽑는것 => 트랜스폼\n",
    "- 100개 종목 얼른 긁고"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. 서비스 한번\n",
    "- 뉴스 한거 돌게 만들고\n",
    "- 워투벡에 뉴스데이터만 넣고\n",
    "2. 모델 퍼포먼스 개선을 위해서 다시 수정\n",
    "- doc2vec도 해보고 mecab으로도 써보고 전처리 재가공\n",
    "\n",
    "3. 다플에 질문과 답을 넣는다\n",
    "-주식에 대해 물어보면 매도할때 세금이 얼마야? 라면 그 매도는 어디에서의 매도냐 주식이니 그걸 context conversation라고 하는데 사용자가 앞 문맥과 동일하다는 걸 명시해줘야 한다.\n",
    "\n",
    "- 챗봇을 선택한 이유와 당위성을 충족히켜야 한다. + 사람들이 챗봇 으로 얻을 수 있는 것을 제공\n",
    "- 스몰톡 자유로운 대화 유도)ux), 대화 시나리오 (대화의 소ㅛㅇ돌이)\n",
    "-다플은 시나리오 가이딩이 안되니 analysis를 근거로 방향성을 제시해서 ~~께 고도화하겠다고 제시 (비전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
