{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 뉴스,주가 데이터 임포트\n",
    "## 전처리 & 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "import datetime\n",
    "from pandas_datareader import data as pdr\n",
    "import fix_yahoo_finance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from eunjeon import Mecab\n",
    "mecab = Mecab()\n",
    "count_vect = CountVectorizer(decode_error=\"replace\")\n",
    "TfidfTransformer()\n",
    "\n",
    "\n",
    "\n",
    "# class 정의 => 기능은 뉴스랑 주가데이터 임포트 ~ 전처리데이터 익스포트\n",
    "# filename : \"news_all.csv\"\n",
    "# filepath : \"./dataset/\"\n",
    "class data_import_preprocess():\n",
    "    def get_allnews(self, filepath):\n",
    "        df = pd.read_csv(filepath, encoding = 'utf-8 sig', engine = 'python')\n",
    "        df['title+body'] = df.apply(lambda x : '{}|||{}'.format(x['TITLE'], x['BODY']), axis = 1) # title과 body 합쳐서 새로운 칼럼 추가  구분자 : |||\n",
    "        df['title+body'] = df['title+body'].astype(str)\n",
    "        df['NOUNS'] = df['title+body'].apply(lambda x : '{}'.format(mecab.nouns(x))) # ''씌워줘야 list 안 원소들이 join해서 붙는다.\n",
    "        df.rename(columns={'TIME': 'Date'}, inplace = True)\n",
    "        df = df.groupby('Date').agg({'NOUNS' : lambda x : ''.join(x)})\n",
    "        return df     \n",
    "\n",
    "\n",
    "    # 1param - 종목코드 / 2param - startdate / 3param - enddate\n",
    "    def stock_get(self, keycode, startdate, enddate):\n",
    "        yf.pdr_override()\n",
    "        df_stock = pdr.get_data_yahoo(keycode, start = startdate, end = enddate)\n",
    "        df_stock['day1T'] = df_stock['Close'].shift(-1)\n",
    "        df_stock['day3T'] = df_stock['Close'].shift(-3)\n",
    "        df_stock['day5T'] = df_stock['Close'].shift(-5)\n",
    "        df_stock['day10T'] = df_stock['Close'].shift(-10)\n",
    "        return df_stock\n",
    "    \n",
    "    \n",
    "    def news_stock_concat(self, news, stock):\n",
    "        news_stock = news.join(df_stock, how = 'outer')\n",
    "        news_stock = news_stock.dropna()\n",
    "        return news_stock\n",
    "\n",
    "\n",
    "    def making_new_columns(self, df):\n",
    "        df['Diff1'] = df['day1T'] - df['Close']\n",
    "        df['Diff3'] = df['day3T'] - df['Close']\n",
    "        df['Diff5'] = df['day5T'] - df['Close']\n",
    "        df['Diff10'] = df['day10T'] - df['Close']\n",
    "        df['Diff1_per'] = round(df['Diff1'] / df['Close'] * 100, 2)\n",
    "        df['Diff3_per'] = round(df['Diff3'] / df['Close'] * 100, 2)\n",
    "        df['Diff5_per'] = round(df['Diff5'] / df['Close'] * 100, 2)\n",
    "        df['Diff10_per'] = round(df['Diff10'] / df['Close'] * 100, 2)\n",
    "        bins = [-100, 0, 100]\n",
    "        labels = [-1, 1]\n",
    "        df['Diff1_clf'] = pd.cut(df[\"Diff1_per\"], bins, labels = labels)\n",
    "        df['Diff3_clf'] = pd.cut(df[\"Diff3_per\"], bins, labels = labels)\n",
    "        df['Diff5_clf'] = pd.cut(df[\"Diff5_per\"], bins, labels = labels)\n",
    "        df['Diff10_clf'] = pd.cut(df[\"Diff10_per\"], bins, labels = labels)\n",
    "        df.drop(['High', 'Low', 'Adj Close', 'Volume'], axis=1, inplace = True)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class stat_modeling():\n",
    "    def CounterVec_tfidf(self, df, filename, n, k, ratio):\n",
    "        # DataFrame에서 필요한 칼럼추출 => split\n",
    "        X = df['NOUNS']\n",
    "        y = df.loc[:, ['Diff' + n + '_clf']]\n",
    "\n",
    "\n",
    "        # 벡터화 (카운트벡터 + tfidf)\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_vec = vectorizer.fit_transform(X)\n",
    "        \n",
    "        \n",
    "        # chi2 변수선택\n",
    "        selector = SelectKBest(chi2, k = k)\n",
    "        X_vec_chi2 = selector.fit_transform(X_vec, y)\n",
    "        \n",
    "        \n",
    "        ##### vec모델 chi2모델 합쳐서 저장 #####\n",
    "        vec_chi2 = {'vectorizer' : vectorizer, 'selector': selector}\n",
    "        with open(filename + 'vec_feature.bin', 'wb') as f:\n",
    "            pickle.dump(vec_chi2, f)\n",
    "\n",
    "\n",
    "        # train / test 나누기\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_vec_chi2, y, test_size = ratio)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    def make_NBmodel(self, X_train_vec, X_test_vec, y_train, y_test, filename):\n",
    "        MultinomialNB(alpha = 1.0, class_prior = None, fit_prior = True)\n",
    "        clf_nb = MultinomialNB()\n",
    "        clf_nb.fit(X_train_vec, y_train)\n",
    "        with open(filename+'NBmodel.pkl', 'wb') as fid:\n",
    "            pickle.dump(clf_nb, fid) \n",
    "        #return NBmodel_Ver1\n",
    "\n",
    "\n",
    "    def make_SVCmodel(self, X_train_vec, X_test_vec, y_train, y_test, filename):        \n",
    "        clf_svc = SVC(probability=True, gamma = 0.1, class_weight = \"balanced\", kernel = 'rbf', C = 10)\n",
    "        clf_svc.fit(X_train_vec, y_train)\n",
    "        with open(filename+'SVCmodel.pkl', 'wb') as fid:\n",
    "            pickle.dump(clf_svc, fid)\n",
    "        #return SVCmodel_Ver1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 클래스 내 메소드 순차적 실행하는 예제코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded\n"
     ]
    }
   ],
   "source": [
    "# 인스턴스 생성\n",
    "cls = data_import_preprocess()\n",
    "\n",
    "\n",
    "df_news = cls.get_allnews('./Dataset/samsung_pre.csv')\n",
    "df_stock = cls.stock_get(\"005930.KS\", \"2017-12-01\", \"2019-01-08\")\n",
    "df_news_stock = cls.news_stock_concat(df_news, df_stock)\n",
    "df_news_stock_new = cls.making_new_columns(df_news_stock)\n",
    "#df_news_stock_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sbg02\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# 이전 모듈 결과값 : df_news_stock_new\n",
    "cls = data_import_preprocess()\n",
    "stat = stat_modeling()\n",
    "\n",
    "X_train, X_test, y_train, y_test = stat.CounterVec_tfidf(df_news_stock_new, '삼성전자', n = '3', k = 15000, ratio = 0.2)\n",
    "\n",
    "stat.make_NBmodel(X_train, X_test, y_train, y_test, '삼성전자') # 파라미터는 기본세팅, 바꾸려면 모듈 고치센\n",
    "stat.make_SVCmodel(X_train, X_test, y_train, y_test, '삼성전자') # 파라미터는 기본세팅, 바꾸려면 모듈 고치센"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 최종결과물 확인\n",
    "- 3만 넘으면 Ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37112"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_news_stock_new)\n",
    "len(df_news_stock_new.iloc[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-1. 통계분류로 예측(SVC, NaiveBaysian)\n",
    "## Vectorization => countervec + tf/idf로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import pickle\n",
    "count_vect = CountVectorizer()\n",
    "TfidfTransformer()\n",
    "\n",
    "# 파이프라인 쓰는법도 고려해보자!!!! 더 쉽고 간결함\n",
    "# 시각화 함수도 넣으면 좋을듯?         \n",
    "        \n",
    "# test만 쓰는데 이 부분을 크롤링 한 뉴스데티어 전처리한 파일 들어가도록 세팅!!!\n",
    "class classification():     \n",
    "    def NB_tfidf(self, X_train_vec, X_test_vec, y_train, y_test, filename):\n",
    "        #clf_nb = load('NBmodel_Ver1') \n",
    "        with open(filename+'NBmodel.pkl', 'rb') as fid:\n",
    "            clf_nb = pickle.load(fid)\n",
    "        \n",
    "        #predicted = clf_tfidf.predict(X_test_vec)\n",
    "        #print('tfidf로 변환한 예측률 :',np.mean(predicted == y_test))\n",
    "        \n",
    "        print('훈련 셋 검증결과 : {:.3f}.'.format(clf_nb.score(X_train, y_train)))\n",
    "        print('테스트 셋 검증결과 : {:.3f}.'.format(clf_nb.score(X_test, y_test)))\n",
    "        #print('UP/Down 예측값 : {:.3f}.'.format(clf_nb.predict_proba(X_test)))  만드는 중\n",
    "        print(clf_nb.predict(X_train))\n",
    "        return clf_nb\n",
    "        \n",
    "\n",
    "    # [과제]gamma는 0.1 / c는 10 디폴트로 설정해보자\n",
    "    # 1param - Xdata / 2param - ydata / 3param - ratio / 4param - N일 뒤 예측(1,3,5,10)\n",
    "    def SVC_tfidf(self, X_train_vec, X_test_vec, y_train, y_test, filename):  # ratio - 0.2 / gamma - 0.1 / c - 10 디폴트하거나 파라미터 삭제\n",
    "        #clf_svc = load('SVCmodel_Ver1') \n",
    "        with open(filename+'SVCmodel.pkl', 'rb') as fid:\n",
    "            clf_svc = pickle.load(fid)\n",
    "            \n",
    "        print('훈련 셋 검증결과 : {:.3f}.'.format(clf_svc.score(X_train_vec, y_train)))\n",
    "        print('테스트 셋 검증결과 : {:.3f}.'.format(clf_svc.score(X_test_vec, y_test)))\n",
    "        #print(clf_svc.predict_proba(X_test))\n",
    "        print(clf_svc.predict(X_train))\n",
    "    \n",
    "    \n",
    "        # 퍼센트 안내멘트를 리턴하면 될듯?\n",
    "        return clf_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 실행1 - countVec + Naive Baysian 분류기\n",
    "- NB는 왠지 피팅만 안되도록 벡터수를 조절하면 예측률이 좀 높아지지 않을까?\n",
    "- 한글설명 : https://datascienceschool.net/view-notebook/c19b48e3c7b048668f2bb0a113bd25f7/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 셋 검증결과 : 0.594.\n",
      "테스트 셋 검증결과 : 0.569.\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# 이전 모듈 결과값 : df_news_stock_new\n",
    "clf = classification()\n",
    "\n",
    "nb = clf.NB_tfidf(X_train, X_test, y_train, y_test, '삼성전자')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 실행2 - countVec + Kernel SVM 분류기\n",
    "- 한글설명 : https://tensorflow.blog/파이썬-머신러닝/2-3-7-커널-서포트-벡터-머신/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 셋 검증결과 : 0.975.\n",
      "테스트 셋 검증결과 : 0.725.\n",
      "[ 1 -1 -1 -1 -1  1  1 -1 -1 -1  1 -1  1  1 -1  1  1 -1 -1  1  1  1 -1 -1\n",
      " -1  1  1  1 -1  1  1 -1  1 -1 -1  1 -1 -1  1 -1 -1 -1 -1 -1  1  1  1 -1\n",
      "  1  1 -1 -1 -1  1 -1  1 -1 -1  1  1 -1  1 -1  1  1 -1  1  1 -1 -1 -1 -1\n",
      " -1 -1  1  1  1  1 -1  1 -1 -1 -1 -1 -1  1  1 -1 -1 -1 -1 -1 -1  1 -1  1\n",
      "  1  1  1 -1 -1 -1 -1 -1  1  1 -1 -1 -1 -1 -1 -1  1  1  1  1 -1 -1  1 -1\n",
      " -1  1 -1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1 -1  1 -1  1  1 -1 -1 -1  1\n",
      "  1 -1 -1 -1  1  1 -1 -1  1  1  1  1  1 -1  1 -1  1  1 -1  1 -1 -1 -1 -1\n",
      "  1 -1 -1  1  1  1  1 -1 -1  1  1 -1  1 -1  1  1  1 -1  1 -1 -1  1 -1  1\n",
      "  1  1 -1 -1 -1 -1 -1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "clf = classification()\n",
    "\n",
    "svc = clf.SVC_tfidf(X_train, X_test, y_train, y_test, '삼성전자')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<51x15000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 32309 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N일치 뉴스로 분류해보기\n",
    "- tfidf 에러 : https://stackoverflow.com/questions/44855780/idf-vector-is-not-fitted-error-when-using-a-saved-classifier-model?rq=1\n",
    "- 피클 에러: http://thiagomarzagao.com/2015/12/08/saving-TfidfVectorizer-without-pickles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인스턴스 생성\n",
    "cls = data_import_preprocess()\n",
    "\n",
    "X = df_news_stock_new['NOUNS']\n",
    "X_new = cls.news_get('./small_news/삼성전자.csv')\n",
    "X_new = X_new.groupby('Date').agg({'NOUNS' : lambda x : ''.join(x)})\n",
    "X_new2 = X_new['NOUNS']\n",
    "X_new2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC분류모델 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류모델과 전처리모델 불러와 저장\n",
    "with open('삼성전자SVCmodel.pkl', 'rb') as f:\n",
    "    clf_svc = pickle.load(f)\n",
    "with open('삼성전자vec_feature.bin', 'rb') as f:\n",
    "    vec_feature = pickle.load(f)\n",
    "\n",
    "# 전처리 피클파일에서 모델 불러와 저장\n",
    "vectorizer = vec_feature.get('vectorizer')\n",
    "selector = vec_feature.get('selector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기존 tf/idf모델, Feature Selection모델 임포트\n",
    "- import : https://stackoverflow.com/questions/33497314/sklearn-dumping-model-using-joblib-dumps-multiple-files-which-one-is-the-corre\n",
    "- dict import : \n",
    "- https://stackoverflow.com/questions/32764991/how-do-i-store-a-tfidfvectorizer-for-future-use-in-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x30648 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1572 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new2_vec = vectorizer.transform(X_new2)\n",
    "X_new2_chi2 = selector.transform(X_new2_vec)\n",
    "predicted = clf_svc.predict(X_new2_chi2)\n",
    "predicted_probability = clf_svc.predict_proba(X_new2_chi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  1, -1], dtype=int64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predicted)\n",
    "print(predicted_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 새로운거 시도하는 중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_indices, validation_indices = training_indices, testing_indices = train_test_split(df.index, stratify = df_class, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from tpot import TPOTRegressor\n",
    "\n",
    "tpot = TPOTClassifier(generations=5,verbosity=2)\n",
    "\n",
    "tpot.fit(df.drop('Diff1_clf', axis=1).loc[training_indices].values,\n",
    "         df.loc[training_indices, 'Diff1_clf'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = df_news_stock_new['Diff1_clf'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_news_stock_new[['NOUNS', 'Diff1_clf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NOUNS'] = df['NOUNS'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_stock_new.drop(df_news_stock_new.columns['Open', 'Close', 'day1T', 'day3T', 'day5T', 'day10T', 'Diff1',\n",
    "       'Diff3', 'Diff5', 'Diff10', 'Diff1_per', 'Diff3_per', 'Diff5_per',\n",
    "       'Diff10_per'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_stock_new.drop(['Open', 'Close', 'day1T', 'day3T', 'day5T', 'day10T', 'Diff1',\n",
    "       'Diff3', 'Diff5', 'Diff10', 'Diff1_per', 'Diff3_per', 'Diff5_per',\n",
    "       'Diff10_per'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_news_stock_new['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-2. 통계분류로 예측(svc, NaiveBaysian)\n",
    "## Vectorization => Doc2Vec으로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 실행3 - Doc2Vec + Kernel SVM 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = Vector()\n",
    "clf = classification()\n",
    "\n",
    "# 모델 임포트하는데 시간 좀 걸립니다. ㅠ / 모델 합치면 6G정도, 웬만한 노트북은 디스크 긁으면서 돌아갑니다.\n",
    "model = vec.Doc_model_import('./Doc2VecClf/doc2vec_no_pos_tagging.model')\n",
    "\n",
    "# 함수에 모델 넣는것도 오래걸린다 ㅠ\n",
    "X_train, X_test, y_train, y_test = vec.Doc_vector(model, df_news_stock_new, n = '5', ratio = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = vec.Doc_vector(model, df_news_stock_new, n = '10', ratio = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = Vector()\n",
    "clf = classification()\n",
    "\n",
    "svc = clf.SVC_tfidf(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 실행4 - Doc2Vec + Naive Baysian 분류기\n",
    "- 코드는 이상 없는데, doc2vec에서 벡터를 추출하면 음수값이 섞여있으서 분류기 학습이 안된다.ㅠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = Vector()\n",
    "clf = classification()\n",
    "\n",
    "svc = clf.NB_tfidf(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#count_vect = CountVectorizer()\n",
    "\n",
    "\n",
    "# 파이프라인 쓰는법도 고려해보자!!!! 더 쉽고 간결함\n",
    "# 시각화 함수도 넣으면 좋읗듯? \n",
    "class VectorFeature():\n",
    "    \n",
    "    def split_vectorize(self, df, n, ratio):\n",
    "        # DataFrame에서 필요한 칼럼추출 => split\n",
    "        X = df['NOUNS']\n",
    "        y = df.loc[:, ['Diff'+n+'_clf']]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = ratio)\n",
    "        \n",
    "        \n",
    "        # 벡터화+tf/idf 해주는 pipeline 설계\n",
    "        makevector = Pipeline([\n",
    "            ('vect', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])\n",
    "        \n",
    "                \n",
    "        # 벡터화 진행 후 저장(X_data만  실시)\n",
    "        X_train_vec = makevector(X_train)\n",
    "        X_test_vec = makevector(X_test)\n",
    "        return X_train_vec, X_test_vec, y_train, y_test\n",
    "    \n",
    "    \n",
    "    # k =15000정도 괜춘\n",
    "    def feature_Select(self, X_train_vec, X_test_vec, y_train, y_test, k):\n",
    "        # X벡터를 y값과 비교해 카이제곱검정으로 벡터 축소\n",
    "        X_train_chi2 = SelectKBest(chi2, k = k).fit_transform(X_train_vec, y_train)\n",
    "        X_test_chi2 = SelectKBest(chi2, k = k).fit_transform(X_test_vec, y_test)\n",
    "        print(X_train_chi2.shape)\n",
    "        print(X_test_chi2.shape)\n",
    "        return X_train_chi2, X_test_chi2\n",
    "    \n",
    "    \n",
    "''' 위 함수에 메소드 오버라이딩해보면 좋을 듯?\n",
    "    super().feature_Select(X_train_vec, y,  k)   이렇게...?\n",
    "\n",
    "\n",
    "    # k =15000정도 괜춘\n",
    "    def feature_Select(self, X_train_vec, X_test_vec, y_train, y_test, k):\n",
    "        # X벡터를 y값과 비교해 카이제곱검정으로 벡터 축소\n",
    "        X_train_chi2 = SelectKBest(chi2, k = k).fit_transform(X_train_vec, y_train)\n",
    "        X_test_chi2 = SelectKBest(chi2, k = k).fit_transform(X_test_vec, y_test)\n",
    "        print(X_train_chi2.shape)\n",
    "        print(X_test_chi2.shape)\n",
    "        return X_train_chi2, X_test_chi2\n",
    "'''\n",
    "    \n",
    "class classification(VectorFeature): \n",
    "    \n",
    "    def NB_tfidf(self, X_train_vec, X_test_vec, y_train, y_test):\n",
    "        MultinomialNB(alpha = 1.0, class_prior = None, fit_prior = True)\n",
    "        clf_nb = MultinomialNB()\n",
    "        clf_nb.fit(X_train_vec, y_train)\n",
    "        \n",
    "        #predicted = clf_tfidf.predict(X_test_vec)\n",
    "        #print('tfidf로 변환한 예측률 :',np.mean(predicted == y_test))\n",
    "        \n",
    "        print('훈련 셋 검증결과 : {:.3f}.'.format(clf_nb.score(X_train, y_train)))\n",
    "        print('테스트 셋 검증결과 : {:.3f}.'.format(clf_nb.score(X_test, y_test)))\n",
    "        #print('UP/Down 예측값 : {:.3f}.'.format(clf_nb.predict_proba(X_test)))  만드는 중\n",
    "        print(clf_nb.predict(X_train))\n",
    "        \n",
    "\n",
    "    # [과제]gamma는 0.1 / c는 10 디폴트로 설정해보자\n",
    "    # 1param - Xdata / 2param - ydata / 3param - ratio / 4param - N일 뒤 예측(1,3,5,10)\n",
    "    def SVC_tfidf(self, X_train_vec, X_test_vec, y_train, y_test):  # ratio - 0.2 / gamma - 0.1 / c - 10 디폴트하거나 파라미터 삭제\n",
    "        clf_svc = SVC(gamma = 0.1, class_weight = \"balanced\", kernel = 'rbf', C = 10)\n",
    "        clf_svc.fit(X_train_vec, y_train)\n",
    "        \n",
    "        print('훈련 셋 검증결과 : {:.3f}.'.format(clf_svc.score(X_train_vec, y_train)))\n",
    "        print('테스트 셋 검증결과 : {:.3f}.'.format(clf_svc.score(X_test_vec, y_test)))\n",
    "        #print(clf_svc.predict_proba(X_test))\n",
    "        print(clf_svc.predict(X_train))\n",
    "    \n",
    "    \n",
    "        # 퍼센트 안내멘트를 리턴하면 될듯?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # param1 - data / param2 - testSet 비율\n",
    "    # X_train과 X_test만 벡터화시키면 된다. y는 그냥 칼럼 슬라이싱만 하면 ok(아직은 y값 2진분류)\n",
    "    def split_vectorize(df, ratio):        \n",
    "        # Train data 벡터 + tf/idf\n",
    "        X_train_counts = count_vect.fit_transform(X) \n",
    "        tfidf_transformer = TfidfTransformer(use_idf=True).fit(X_train_counts) \n",
    "        X_train_tfidf = tfidf_transformer.transform(X_train_counts)\n",
    "        \n",
    "        # Test data 벡터 + tf/idf\n",
    "        X_test_counts = count_vect.transform(X)\n",
    "        X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "        return (X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "        \n",
    "    # 아직 y슬라이싱 안하고 집어넣음 =-> 수정 보완 ㄱㄱㄱㄱㄱㄱㄱㄱㄱㄱ \n",
    "    def NB_tfidf(df):\n",
    "        clf_train = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "        predicted_train = clf_tfidf.predict(X_train_tfidf)\n",
    "        print('======== Train데이터 예측률 ========')\n",
    "        print('tfidf로 변환한 예측률 :', np.mean(predicted_train == y_train))\n",
    "        \n",
    "        clf_test = MultinomialNB().fit(X_test_tfidf, y_test)\n",
    "        print('======== Test데이터 예측률 ========')\n",
    "        print('tfidf로 변환한 예측률 :', np.mean(clf_test == y_test))\n",
    "        return 뭘 뽑아야 할까요`~?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
